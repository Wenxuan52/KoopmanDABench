# CAE-MLP Configuration File

# Dataset configuration
dataset:
  dataset_name: cylinder  # Options: kolmogorov, cylinder, chap
  data_path: ./data
  batch_size: 32
  train_ratio: 0.8
  num_workers: 4
  shuffle_train: true
  shuffle_val: false
  drop_last: true
  pin_memory: true
  normalize: true
  random_seed: 42
  
  # # Dataset-specific parameters
  # For Cylinder dataset
  # target_resolution: [64, 64]  # [H, W] for interpolation
  # interpolation_mode: bilinear
  
  # # For CHAP dataset
  # chemical: Cl  # Options: Cl, NH4, NO3, SO4
  # target_shape: [128, 128]  # Required for CHAP to reshape flattened data

# Model configuration
model:
  type: CAE_LinearMLP  # Options: CAE_LinearMLP, CAE_WeakLinearMLP
  params:
    latent_dim: 64
    cae_depth: 4
    cae_base_channels: 32
    use_bn: true
    
    # # For CAE_WeakLinearMLP only
    # hidden_dims: [128, 128]  # Hidden dimensions for nonlinear MLP
    # mlp_activation: tanh  # Options: relu, tanh, gelu
    # mlp_dropout: 0.1

# Training configuration
training:
  train_mode: multiply  # Options: separately, jointly, multiply
  epochs: 5000  # Total epochs for jointly mode, or epochs per stage for multi-stage modes
  
  # Stage-specific epochs (only used for separately and multiply modes)
  stage1_epochs: 5000   # CAE reconstruction pretraining
  stage2_epochs: 5000   # Linear predictor training
  stage3_epochs: 5000   # Joint fine-tuning (multiply mode only)
  
  learning_rate: 0.001
  device: cpu  # Options: cuda, cpu
  save_path: ./results/checkpoints/CAE_MLP
  verbose: true
  
  # Optimizer configuration
  optimizer:
    type: Adam  # Options: Adam, AdamW, SGD
    params:
      betas: [0.9, 0.999]
      eps: 1.0e-08
      weight_decay: 0.0001
  
  # Learning rate scheduler (optional)
  scheduler:
    type: StepLR  # Options: StepLR, ExponentialLR, CosineAnnealingLR, ReduceLROnPlateau
    params:
      # For StepLR
      step_size: 500
      gamma: 0.95
      
      # For ExponentialLR
      # gamma: 0.95
      
      # For CosineAnnealingLR
      # T_max: 100
      # eta_min: 1.0e-06
      
      # For ReduceLROnPlateau
      # mode: min
      # factor: 0.5
      # patience: 10
      # min_lr: 1.0e-06
  
  # Loss function configuration
  loss:
    recon_weight: 1.0      # Weight for reconstruction loss
    pred_weight: 1.0       # Weight for prediction loss
    latent_weight: 0.01    # Weight for latent space regularization
    linear_weight: 0.1     # Weight for linear constraint (weak linear model only)


# ## Training Mode Configurations

# ### Separately Mode
# Train CAE and linear predictor in two separate stages:
# ```yaml
# training:
#   train_mode: separately
#   stage1_epochs: 80   # Stage 1: CAE reconstruction only
#   stage2_epochs: 40   # Stage 2: Linear predictor with frozen CAE
#   learning_rate: 0.001
  
#   # Can use different learning rates for different stages
#   # stage1_lr: 0.001
#   # stage2_lr: 0.0001
# ```

# ### Jointly Mode
# Train CAE and predictor together from the start:
# ```yaml
# training:
#   train_mode: jointly
#   epochs: 100
#   learning_rate: 0.001
  
#   loss:
#     recon_weight: 1.0
#     pred_weight: 1.0
#     latent_weight: 0.01
# ```

# ### Multiply Mode
# Three-stage training with final joint fine-tuning:
# ```yaml
# training:
#   train_mode: multiply
#   stage1_epochs: 60   # Stage 1: CAE reconstruction
#   stage2_epochs: 30   # Stage 2: Linear predictor
#   stage3_epochs: 30   # Stage 3: Joint fine-tuning
#   learning_rate: 0.001
  
#   # The joint fine-tuning stage often benefits from lower learning rate
#   # stage3_lr: 0.0001
# ```

# ### Kolmogorov Dataset Configuration
# ```yaml
# dataset:
#   name: kolmogorov
#   data_path: ./data
#   batch_size: 32
#   # No additional parameters needed

# model:
#   type: CAE_LinearMLP
#   params:
#     latent_dim: 64
#     cae_depth: 4
#     cae_base_channels: 32
# ```

# ### Cylinder Dataset Configuration
# ```yaml
# dataset:
#   name: cylinder
#   data_path: ./data
#   batch_size: 16
#   target_resolution: [64, 256]  # Downsample for efficiency
#   interpolation_mode: bilinear

# model:
#   type: CAE_WeakLinearMLP
#   params:
#     latent_dim: 128  # Larger latent dim for complex flow
#     cae_depth: 5
#     cae_base_channels: 64
#     hidden_dims: [256, 256]
#     mlp_activation: gelu
# ```

# ### CHAP Dataset Configuration
# ```yaml
# dataset:
#   name: chap
#   data_path: ./data
#   batch_size: 8
#   chemical: NO3
#   target_shape: [96, 96]  # Define spatial structure

# model:
#   type: CAE_LinearMLP
#   params:
#     latent_dim: 32
#     cae_depth: 4
#     cae_base_channels: 32

# ## Usage Notes

# 1. **Memory Considerations**: 
#    - Reduce `batch_size` if running out of GPU memory
#    - Reduce `target_resolution` for Cylinder dataset if needed
#    - Reduce `cae_base_channels` or `cae_depth` for smaller models

# 2. **Performance Tips**:
#    - Set `num_workers` based on your CPU cores (usually 4-8)
#    - Enable `pin_memory` when using GPU
#    - Use `drop_last=true` for stable training with batch normalization

# 3. **Model Selection**:
#    - Use `CAE_LinearMLP` for simpler dynamics or when interpretability is important
#    - Use `CAE_WeakLinearMLP` for complex nonlinear dynamics while maintaining some linear structure

# 4. **Hyperparameter Tuning**:
#    - Adjust `latent_dim` based on data complexity (32-128 typical)
#    - Tune loss weights based on validation performance
#    - Start with lower learning rate (1e-4) for stability